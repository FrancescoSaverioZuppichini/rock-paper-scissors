{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a convnet for ✊✋✌\n",
    "This presents how to build a convnet from scratch to classify images of rock-paper-scissors.  It is meant as a teaching activity to demonstrate the following concepts in practice:\n",
    "- how images are represented and handled in software\n",
    "- how to prepare a machine learning dataset\n",
    "- how a full machine learning pipeline looks\n",
    "- data preprocessing\n",
    "- data augmentation and its importance in a \n",
    "- overfitting, underfitting\n",
    "\n",
    "We use the high-level deep learning library Keras, but the concepts are general and we don't put much focus on the specifics of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 0: defining the problem\n",
    "What problem do we want to solve, exactly?  We want to build a piece of software that, given an image as input that represents an hand making one of the three ✊✋✌ gestures, produces as output a classification of the image in one of the three classes.\n",
    "\n",
    "In the following, we will adopt this convention\n",
    "- class 0 is ✊ rock\n",
    "- class 1 is ✋ paper\n",
    "- class 2 is ✌ scissors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: building a dataset\n",
    "We are starting from scratch, so we need to shoot our own dataset; the best option is that multiple students/groups shoot plenty of images in parallel and then the data is somehow collected.  Whatever the process, in the end we want to have all pics in three different directories, one per class.  Format can be either jpg or png, and landscape/portrait, aspect ratio and resolution don't matter and can be mixed.\n",
    "\n",
    "With some attention to logistics, this can be done in about 10-30 minutes.\n",
    "\n",
    "Guidelines for shooting images. \n",
    "- We don't need high resolution: use the lowest resolution/quality allowed by the phone (this reduces the size of the dataset and speeds up data transfer).\n",
    "- The hand must be more or less in the center of the image; it should not fill the whole image, but it should not be too small either.  ![caption](figures/guidelines.jpg)\n",
    "- we want the dataset to represent as much variability as possible: if we want the classifier to work for all hand orientations, try to have examples for all of them; if we want to handle many different lightling conditions, try to have some pictures for different lightings;\n",
    "- avoid poses that are ambiguous, unless you want to make your job harder: e.g., don't include in the dataset images of paper or scissors taken from the side;\n",
    "- avoid having two images in the dataset that are almost the same: change the camera and hand pose at least a little bit; this is important because in the following code we randomly split training and testing data.\n",
    "\n",
    "Remember that we need the images for each class to be in its own directory. To make this simpler, it helps to shoot first all images of rock, then all images of paper, then all images of scissors, and finally sort the images by time in the file manager and group them accordingly.\n",
    "\n",
    "Place all images in three directories named `c0/`, `c1/`, and `c2/`.  Make sure that each directory only contains image files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: read in images and have a look at them\n",
    "Let's first import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabrizio\\Anaconda3\\lib\\site-packages\\skimage\\viewer\\utils\\core.py:10: UserWarning: Recommended matplotlib backend is `Agg` for full skimage.viewer functionality.\n",
      "  warn(\"Recommended matplotlib backend is `Agg` for full \"\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8504cf221c73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Setup to show interactive jupyter widgets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import skimage.viewer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import keras.utils.np_utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup to show interactive jupyter widgets\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "def imgplotList(i,data):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(data[i],interpolation=\"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define where datasets are located\n",
    "dataset_directory = pathlib.Path(\"..\")/\"datasets\"/\"final\"\n",
    "\n",
    "# Define which datasets we should consider.\n",
    "# Each dataset is a directory withing dataset_directory\n",
    "# and must contain three subdirectories: (c0, c1, c2) for (rock, paper, scissors).\n",
    "dnames = [\"D{}\".format(n) for n in range(1,8)] + [\"testing\"]\n",
    "\n",
    "\n",
    "# Now check the data\n",
    "ddirs=[dataset_directory/dn for dn in dnames] # directories of the dataset\n",
    "cdirs={}\n",
    "for ddir in ddirs:\n",
    "    cdirs.update({ddir/\"c0\":0,\n",
    "                  ddir/\"c1\":1,\n",
    "                  ddir/\"c2\":2})\n",
    "names = [\"rock\", \"paper\", \"scissors\"]\n",
    "for cdir,cdir_class in cdirs.items():\n",
    "    assert(cdir.exists())\n",
    "    print(\"Found directory {} containing class {}\".format(cdir,names[cdir_class]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try to read the first image from the first directory, and visualize it.  Note that the tool allows you to zoom in order to see the individual pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cdirs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7572ef90d3d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcdirs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mviewer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageViewer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Note: you have to close the window to continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cdirs' is not defined"
     ]
    }
   ],
   "source": [
    "im = skimage.io.imread(list(list(cdirs.keys())[0].glob(\"*\"))[0])\n",
    "viewer=skimage.viewer.ImageViewer(im)\n",
    "viewer.show()\n",
    "# Note: you have to close the window to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now read in all images, cut the central square (with an edge as long as the shorter dimension), and resize it to 200x200 pixels.  Whatever the initial size and orientation of the images, we will end up with a bunch of 200x200 RGB squares in uint8.  These should be small enough that unless the dataset is huge, all should fit in memory.\n",
    "\n",
    "We make a pandas dataframe with the data, with two columns:\n",
    "* image: a $200 \\times 200 \\times 3$ uint8 numpy array\n",
    "* label: on of 0, 1 or 2\n",
    "* file: the full path of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imagesize = 200\n",
    "\n",
    "dataset=[]\n",
    "\n",
    "import warnings\n",
    "\n",
    "for cdir,cn in tqdm(list(cdirs.items())):\n",
    "    for f in list(cdir.glob(\"*\")):\n",
    "        try:\n",
    "            im=skimage.io.imread(f)\n",
    "        except (OSError, ValueError) as e:\n",
    "            warnings.warn(\"ignoring {} due to exception {}\".format(f,str(e)))\n",
    "            continue\n",
    "            \n",
    "        h,w=im.shape[0:2] # height, width\n",
    "        sz=min(h,w)\n",
    "        im=im[(h//2-sz//2):(h//2+sz//2),(w//2-sz//2):(w//2+sz//2),:] # defines the central square        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            im=skimage.img_as_ubyte(skimage.transform.resize(im,(imagesize,imagesize))) # resize it to 500x500, whatever the original resolution\n",
    "            \n",
    "        dataset.append({\n",
    "            \"file\":f,\n",
    "            \"label\":cn,\n",
    "            \"image\":im})\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We make a pandas dataframe for the dataset, and create a \"dn\" field containing the name of the dataset from which each image comes (as the name of the directory it was read from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame(dataset)\n",
    "dataset[\"dn\"]=dataset[\"file\"].apply(lambda x: x.parent.parts[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here are 10 random rows from that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.sample(n=10)[[\"image\",\"label\",\"file\",\"dn\"]].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optionally, we can quickly scroll through the images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viewer=skimage.viewer.CollectionViewer([r[1][\"image\"] for r in dataset.iterrows()])\n",
    "viewer.show()\n",
    "# Note: you have to close the window to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Prepare training and testing sets\n",
    "\n",
    "How should we split training and testing data?  The code below implements a few options (run only one of the cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Option one (hard): use all samples from dataset named \"testing\"\n",
    "# which contains some images taken in the same days as D1--D4, but not contained in these dirs.\n",
    "te_mask = dataset[\"dn\"]==\"testing\"\n",
    "dataset_te=dataset[te_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Option two (hard): test on all samples from one specific dataset\n",
    "te_mask = dataset[\"dn\"]==\"D2\"\n",
    "dataset_te=dataset[te_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# In any case, training samples are all other samples\n",
    "dataset_tr=dataset.loc[dataset.index.difference(dataset_te.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Print a summary of how many training and testing images we have sampled\n",
    "import collections\n",
    "pd.DataFrame(index=[0,1,2],data=collections.OrderedDict((\n",
    "    (\"Class name\",           names),\n",
    "    (\"# Training images\", dataset_tr[\"label\"].value_counts()),\n",
    "    (\"# Testing images\",  dataset_te[\"label\"].value_counts())))).set_index(\"Class name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "examples=list(dataset_te[\"image\"])\n",
    "interact(\n",
    "    imgplotList, \n",
    "    i=widgets.IntSlider(min=0,max=len(examples)-1, step=1, value=0,continuous_update=True), \n",
    "    data=fixed(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: define what we feed to the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "im = dataset_tr.sample(1).iloc[0][\"image\"]\n",
    "print(im.shape, im.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Take image and resize to a specified size\n",
    "def transform_simple(im,sz):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        imr = skimage.transform.resize(im, (sz,sz))\n",
    "    return imr\n",
    "\n",
    "transform = transform_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Take image and resize to a specified size, after applying data augmentation\n",
    "def transform_complex(im,sz):\n",
    "    if(np.random.rand()<0.5):\n",
    "        im=np.fliplr(im)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        tf1 = skimage.transform.SimilarityTransform(scale = 1 / im.shape[0])\n",
    "        tf2 = skimage.transform.SimilarityTransform(translation=[-0.5, -0.5])\n",
    "        tf3 = skimage.transform.SimilarityTransform(rotation=np.deg2rad(np.random.uniform(0,360)))\n",
    "        tf4 = skimage.transform.SimilarityTransform(scale=np.random.uniform(1,1.6))\n",
    "        tf5 = skimage.transform.SimilarityTransform(translation=np.array([0.5, 0.5])+np.random.uniform(-0.1,0.1,size=2))\n",
    "        tf6 = skimage.transform.SimilarityTransform(scale=sz)\n",
    "        imr = skimage.transform.warp(im, (tf1+(tf2+(tf3+(tf4+(tf5+tf6))))).inverse, output_shape=(sz,sz),mode=\"edge\")\n",
    "        imr = imr*np.random.uniform(0.9,1.1,size=(1,1,3))\n",
    "        imr = np.clip(imr,0,1)\n",
    "    return imr\n",
    "\n",
    "transform = transform_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The resolution challenge\n",
    "@interact(sz = widgets.IntSlider(min=2,max=100),\n",
    "          seed = widgets.IntSlider(min=0,max=100),\n",
    "          reveal = widgets.widgets.ToggleButton(value=False,description='Reveal'))\n",
    "def f(sz,seed,reveal):\n",
    "    fig,axs = plt.subplots(nrows = 2, ncols = 5,figsize=(6,3),dpi=150)\n",
    "    ims = dataset_tr.sample(len(axs.flatten()), random_state=sz*100+seed)\n",
    "    for ax,(_,row) in zip(axs.flatten(),ims.iterrows()):\n",
    "        ax.imshow(transform_simple(row[\"image\"],sz))\n",
    "        ax.axis(\"off\")\n",
    "        if(reveal):\n",
    "            ax.set_title(names[row[\"label\"]])\n",
    "    #fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sample(df,sz):\n",
    "    r=df.sample(n=1)\n",
    "    l=r[\"label\"].iloc[0]\n",
    "    im=r[\"image\"].iloc[0]\n",
    "    im=transform(im,sz)\n",
    "    return im,l\n",
    "\n",
    "def mkbatch(df,N,sz):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        im,l=sample(df,sz)\n",
    "        X.append(im)\n",
    "        y.append(l)\n",
    "    X=np.array(X).astype('float32')\n",
    "    y=np.array(y)\n",
    "    y=keras.utils.np_utils.to_categorical(y,3)\n",
    "    return X,y\n",
    "\n",
    "def generator(df,batch_size,sz):\n",
    "    while True:\n",
    "        X,y = mkbatch(df,batch_size,sz)\n",
    "        yield (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "b = mkbatch(dataset_tr,20,32)\n",
    "b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize 100 images from our input dataset\n",
    "examples = list(mkbatch(dataset_tr,100,32)[0])\n",
    "interact(imgplotList, i=widgets.IntSlider(min=0, max=len(examples)-1, step=1, value=0,continuous_update=False), data=fixed(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize 100 variations from our first input image (makes sense only if transform==transform_complex, i.e. if we are using data augmentation)\n",
    "examples = list(mkbatch(dataset_tr.iloc[[0]],100,32)[0])\n",
    "interact(imgplotList, i=widgets.IntSlider(min=0, max=len(examples)-1, step=1, value=0,continuous_update=False), data=fixed(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5: build and train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras: Deep Learning library for Theano and TensorFlow\n",
    "import keras\n",
    "from keras.utils  import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dropout\n",
    "\n",
    "# NN settings\n",
    "patchsize          = 32\n",
    "batch_size         = 32\n",
    "pool_size          = (2,2) # size of pooling area for max pooling\n",
    "kernel_size        = (3,3) # convolution kernel size\n",
    "\n",
    "def makeModel(nb_filters):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(nb_filters, kernel_size, input_shape=(patchsize,patchsize,3), padding = \"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = pool_size))\n",
    "    model.add(Conv2D(nb_filters*2, kernel_size, padding = \"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = pool_size))\n",
    "    model.add(Conv2D(nb_filters*4, kernel_size, padding = \"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = pool_size))\n",
    "    model.add(AveragePooling2D(pool_size = pool_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128)) # generate a fully connected layer wiht 128 outputs (arbitrary value)\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3)) # output layer\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    ## compile! network\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeModel(128).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Build a set of 1000 testing instances taken from the testing dataset.\n",
    "\n",
    "Note: \"testing\" in this case is synonym with \"validation\" and \"evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_test,y_test) = mkbatch(dataset_te, 1000, patchsize)\n",
    "\n",
    "# Prepare the logs directory, if it does not exist\n",
    "(pathlib.Path(\".\")/\"logs\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While the network trains, we can monitor training loss/accuracy and testing loss/accuracy using tensorboard at http://0.0.0.0:6006\n",
    "\n",
    "You may need to launch tensorboard first if it's not already running, by executing\n",
    "\n",
    "`tensorboard --logdir=logs`\n",
    "\n",
    "in a shell with the current working directory.  Check that you are within the proper conda environment, if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelid = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=50),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='model_checkpoint_best_{}.h5'.format(modelid),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='./logs/'+modelid,\n",
    "        histogram_freq=0, write_graph=False, write_images=False)\n",
    "]\n",
    "\n",
    "model = makeModel(32)\n",
    "history=model.fit_generator(\n",
    "                    generator(dataset_tr, batch_size, patchsize),\n",
    "                    steps_per_epoch=50, \n",
    "                    epochs=5000, \n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test,y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After training our model, we can save it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelname = \"input32\"\n",
    "\n",
    "# Save model to a file\n",
    "keras.models.save_model(model,\"{}.model\".format(modelname))\n",
    "\n",
    "# Also save the testing dataset (may be large) so we can pick up from here later \n",
    "dataset_te.to_pickle(\"{}.testingdata.pickle\".format(modelname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you have time and want to experiment, you may train many networks exploring the parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train many models\n",
    "for filters in [1,2,4,8,16,32,48,64,96]:\n",
    "    modelid = \"filters{:03d}_timestamp{}\".format(filters,time.strftime(\"%Y%m%d%H%M%S\"))\n",
    "\n",
    "    callbacks_list = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_acc',\n",
    "            patience=50),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath='model_checkpoint_best_{}.h5'.format(modelid),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True),\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir='./logs/'+modelid,\n",
    "            histogram_freq=0, write_graph=False, write_images=False)\n",
    "    ]\n",
    "    \n",
    "    model = makeModel(filters)\n",
    "    print(model.summary())\n",
    "    print(model.count_params())\n",
    "\n",
    "    history=model.fit_generator(\n",
    "                        generator(dataset_tr, batch_size, patchsize),\n",
    "                        steps_per_epoch=50, \n",
    "                        epochs=5000, \n",
    "                        verbose=1,\n",
    "                        validation_data=(X_test,y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 6: Process the images of the testing set one by one\n",
    "And visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optionally, load a saved model and testing dataset\n",
    "modelname = \"models/model_venus\"\n",
    "\n",
    "model = keras.models.load_model(\"{}.model\".format(modelname))\n",
    "patchsize = model.input.shape[1].value\n",
    "dataset_te = pd.read_pickle(\"{}.testingdata.pickle\".format(modelname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show results by processing a single variation of the testing image\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "def resultsShow(i, data, model):\n",
    "    d = data.iloc[i]\n",
    "    im = d[\"image\"]\n",
    "    l = d[\"label\"]\n",
    "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(15,5),gridspec_kw={'width_ratios':[1,1,0.5]})\n",
    "    \n",
    "    imt = transform_simple(im, patchsize)\n",
    "    axs[0].imshow(im)\n",
    "    axs[0].set_title(\"Image (true class: {})\".format(names[l]))\n",
    "    \n",
    "    axs[1].imshow(imt,interpolation=\"nearest\")\n",
    "    axs[1].set_title(\"Network input\")\n",
    "    \n",
    "    outs = model.predict(np.array([imt]))\n",
    "    print(outs)\n",
    "    predicted = np.argmax(outs)\n",
    "    axs[2].bar(np.array(range(len(names))), outs[0,:], 1, color=\"gray\")\n",
    "    axs[2].set_ylim([0,1])\n",
    "    axs[2].set_xticks(range(len(names)))\n",
    "    axs[2].set_xticklabels(names)\n",
    "    axs[2].set_ylabel(\"probability\")\n",
    "    axs[2].set_xlabel(\"class\")\n",
    "    axs[2].set_title(\"Network output\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(\"out_{:05d}_{}.png\".format(i,(\"ok\" if predicted==l else \"ko\")))\n",
    "    print(outs)\n",
    "\n",
    "interact(resultsShow, i=widgets.IntSlider(min=0,max=len(dataset_te)-1, step=1, value=0, continuous_update=False), data=fixed(dataset_te.sample(len(dataset_te))), model=fixed(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's visualize the filters learned by the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].get_weights()[0][:,:,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize filters in a layer\n",
    "import itertools\n",
    "filters = model.layers[2].get_weights()[0]\n",
    "filters.shape\n",
    "fig, axs = plt.subplots(nrows = filters.shape[2], ncols = filters.shape[3], figsize=(20,10))\n",
    "for i,j in itertools.product(range(filters.shape[2]),range(filters.shape[3])):\n",
    "    axs[i,j].imshow(filters[:,:,i,j],vmin=-0.5,vmax=+0.5,cmap=\"gray\")\n",
    "    axs[i,j].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's visualize the activations in the intermediate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def plot_hidden_layers(imt,nmaps=4):\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    plt.title('Hidden layers', loc='center')\n",
    "    plt.axis('off')\n",
    "    layers = [model.layers[0].input] + [model.layers[i].output for i in [0,2,5,7,9]]\n",
    "    for layeri,layer in enumerate(layers):\n",
    "        get = K.function([model.layers[0].input], [layer])\n",
    "        layeroutputs = get([imt[np.newaxis,:,:,:]])[0][0]\n",
    "        for j in range(nmaps): # for each map\n",
    "            sp2=fig.add_subplot(nmaps,len(layers),layeri+1+j*len(layers))\n",
    "            sp2.axis('off')\n",
    "            if (layeroutputs.shape[2]>j):\n",
    "                vmin,vmax = ((0,1) if layeri == 0 else (-0.3,+0.3))\n",
    "                sp2.imshow(layeroutputs[:,:,j],\n",
    "                           cmap=\"gray\",\n",
    "                           interpolation=\"nearest\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "imt=transform(dataset_te[\"image\"].iloc[0],patchsize)\n",
    "plot_hidden_layers(imt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We now search for the inputs maximizing a given neuron or set of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def makeinputmaximizing(loss, gradstep = 0.01, steps = 20):\n",
    "    grads = K.gradients(loss, model.input)[0]\n",
    "    grads /= K.sqrt(K.mean(K.square(grads)) + 1e-5)\n",
    "    iterate = K.function([model.input],[loss, grads])\n",
    "\n",
    "    # img = transform_simple(dataset_te[\"image\"].iloc[0],patchsize)[np.newaxis,:,:,:]\n",
    "    img = 0.5+0.01*np.random.rand(1,patchsize,patchsize,3)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        loss_value, grads_value = iterate([img])\n",
    "        img += grads_value * gradstep\n",
    "        img = np.clip(img,0,1)\n",
    "    return img[0,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(makeinputmaximizing(model.layers[16].output[0,2], steps = 1000, gradstep = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "out = model.layers[12].output\n",
    "fig, axs = plt.subplots(nrows = 2, ncols = 10, figsize=(20,5), squeeze = False)\n",
    "for i,j in tqdm.tqdm(list(itertools.product(range(axs.shape[0]), range(axs.shape[1])))):\n",
    "    loss = K.mean(out[:,j])\n",
    "    img = makeinputmaximizing(loss, gradstep = 0.03, steps = 20)\n",
    "    axs[i,j].imshow(img)\n",
    "    axs[i,j].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let us see what the convnet thinks is an ideal paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "iterations=20\n",
    "\n",
    "input_shape=model.input.shape[1]\n",
    "nc=model.output_shape[1]\n",
    "dense2_name=model.layers[15].name\n",
    "final_output=np.zeros((input_shape*nc, input_shape*iterations, model.input.shape[3]))\n",
    "\n",
    "def ideal():\n",
    "    B=64\n",
    "    for k in range(nc):\n",
    "        step=1\n",
    "        X=np.random.random((1, input_shape, input_shape, model.input.shape[3]))*0.1+0.5\n",
    "        ideal=0\n",
    "        objective = model.output[0,k]        \n",
    "        c=K.gradients(objective, model.input)[0]\n",
    "        get=K.function([model.input, K.learning_phase()],[objective, c])\n",
    "        A=0\n",
    "        \n",
    "        for j in range(iterations):\n",
    "            loss_value, grads_value=get([X, 1])\n",
    "            final_output[input_shape*k:input_shape*(k+1),A:A+B]=X[0]\n",
    "            step=1/np.max(grads_value)            \n",
    "            X += grads_value*step\n",
    "            A+=B\n",
    "            X=np.clip(X, 0, 1)\n",
    "    plt.imshow(final_output[:,:,:], vmin=0, vmax=1)\n",
    "    \n",
    "ideal()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let us locate the gesture in the image\n",
    "- First, we feed an image of a paper gesture to the network;\n",
    "- Second, we assign to each channel in the output of the last convolution a value corresponding ot its contribution to the class \"paper\";\n",
    "- Third, we consider which parts in the input image activate the channels that lead to the decision: this is a paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img=skimage.img_as_float(skimage.io.imread(\"paper.jpg\"))\n",
    "img=np.expand_dims(img, axis=0)\n",
    "preds=model.predict(img)\n",
    "\n",
    "rps_output=model.output[:, np.argmax(preds[0])]\n",
    "last_conv_layer=model.get_layer('conv2d_12')\n",
    "grads=K.gradients(rps_output, last_conv_layer.output)[0]\n",
    "pooled_grads=K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "iterate=K.function([model.input, K.learning_phase()], [pooled_grads, last_conv_layer.output[0]])\n",
    "pooled_grads_value, conv_layer_output_value = iterate([img, 1])\n",
    "##\n",
    "for i in range(5):\n",
    "    conv_layer_output_value[:,:,i] *= pooled_grads_value[i]\n",
    "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "heatmap=cv2.resize(heatmap, (img.shape[1], img.shape[2]))\n",
    "\n",
    "heatmap=np.expand_dims(heatmap, axis=3)\n",
    "\n",
    "superimposed_img= np.clip(img + heatmap * 0.4, 0, 1)\n",
    "plt.imshow(superimposed_img)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "livereveal": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
