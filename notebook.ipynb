{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a convnet for ✊✋✌\n",
    "This presents how to build a convnet from scratch to classify images of rock-paper-scissors.  It is meant as a teaching activity to demonstrate the following concepts in practice:\n",
    "- how images are represented and handled in software\n",
    "- how to prepare a machine learning dataset\n",
    "- how a full machine learning pipeline looks\n",
    "- data preprocessing\n",
    "- data augmentation and its importance in a \n",
    "- overfitting, underfitting\n",
    "\n",
    "We use the high-level deep learning library Keras, but the concepts are general and we don't put much focus on the specifics of the code.\n",
    "\n",
    "## Step 0: defining the problem\n",
    "What problem do we want to solve, exactly?  We want to build a piece of software that, given an image as input that represents an hand making one of the three ✊✋✌ gestures, produces as output a classification of the image in one of the three classes.\n",
    "\n",
    "In the following, we will adopt this convention\n",
    "- class 0 is ✊ rock\n",
    "- class 1 is ✋ paper\n",
    "- class 2 is ✌ scissors\n",
    "\n",
    "## Step 1: building a dataset\n",
    "We are starting from scratch, so we need to shoot our own dataset; the best option is that multiple students/groups shoot plenty of images in parallel and then the data is somehow collected.  Whatever the process, in the end we want to have all pics in three different directories, one per class.  Format can be either jpg or png, and landscape/portrait, aspect ratio and resolution don't matter and can be mixed.\n",
    "\n",
    "With some attention to logistics, this can be done in about 10-30 minutes.\n",
    "\n",
    "Guidelines for shooting images. \n",
    "- We don't need high resolution: use the lowest resolution/quality allowed by the phone (this reduces the size of the dataset and speeds up data transfer).\n",
    "- The hand must be more or less in the center of the image; it should not fill the whole image, but it should not be too small either.  ![caption](figures/guidelines.jpg)\n",
    "- we want the dataset to represent as much variability as possible: if we want the classifier to work for all hand orientations, try to have examples for all of them; if we want to handle many different lightling conditions, try to have some pictures for different lightings;\n",
    "- avoid poses that are ambiguous, unless you want to make your job harder: e.g., don't include in the dataset images of paper or scissors taken from the side;\n",
    "- avoid having two images in the dataset that are almost the same: change the camera and hand pose at least a little bit; this is important because in the following code we randomly split training and testing data.\n",
    "\n",
    "Remember that we need the images for each class to be in its own directory. To make this simpler, it helps to shoot first all images of rock, then all images of paper, then all images of scissors, and finally sort the images by time in the file manager and group them accordingly.\n",
    "\n",
    "Place all images in three directories named `c0/`, `c1/`, and `c2/`.  Make sure that each directory only contains image files.\n",
    "\n",
    "## Step 2: read in images and have a look at them\n",
    "Let's first import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib nbagg\n",
    "\n",
    "# General imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import skimage.viewer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import keras.utils.np_utils\n",
    "\n",
    "# Setup to show interactive jupyter widgets\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "def imgplotList(i,data):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(data[i],interpolation=\"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory datasets\\final split\\D2\\c0 containing class rock\n",
      "Found directory datasets\\final split\\D1\\c1 containing class paper\n",
      "Found directory datasets\\final split\\D4\\c2 containing class scissors\n",
      "Found directory datasets\\final split\\testing\\c1 containing class paper\n",
      "Found directory datasets\\final split\\D3\\c0 containing class rock\n",
      "Found directory datasets\\final split\\D2\\c1 containing class paper\n",
      "Found directory datasets\\final split\\D3\\c2 containing class scissors\n",
      "Found directory datasets\\final split\\D5\\c0 containing class rock\n",
      "Found directory datasets\\final split\\D5\\c1 containing class paper\n",
      "Found directory datasets\\final split\\testing\\c0 containing class rock\n",
      "Found directory datasets\\final split\\D4\\c1 containing class paper\n",
      "Found directory datasets\\final split\\D1\\c2 containing class scissors\n",
      "Found directory datasets\\final split\\D2\\c2 containing class scissors\n",
      "Found directory datasets\\final split\\D4\\c0 containing class rock\n",
      "Found directory datasets\\final split\\D1\\c0 containing class rock\n",
      "Found directory datasets\\final split\\D5\\c2 containing class scissors\n",
      "Found directory datasets\\final split\\testing\\c2 containing class scissors\n",
      "Found directory datasets\\final split\\D3\\c1 containing class paper\n"
     ]
    }
   ],
   "source": [
    "# Define where datasets are located\n",
    "dataset_directory = pathlib.Path(\".\")/\"datasets\"/\"final split\"\n",
    "\n",
    "# Define which datasets we should consider.\n",
    "# Each dataset is a directory withing dataset_directory\n",
    "# and must contain three subdirectories: (c0, c1, c2) for (rock, paper, scissors).\n",
    "dnames = [\"D1\",\"D2\",\"D3\",\"D4\",\"D5\",\"testing\"]\n",
    "\n",
    "# Now check the data\n",
    "ddirs=[dataset_directory/dn for dn in dnames] # directories of the dataset\n",
    "cdirs={}\n",
    "for ddir in ddirs:\n",
    "    cdirs.update({ddir/\"c0\":0,\n",
    "                  ddir/\"c1\":1,\n",
    "                  ddir/\"c2\":2})\n",
    "names = [\"rock\", \"paper\", \"scissors\"]\n",
    "for cdir,cdir_class in cdirs.items():\n",
    "    assert(cdir.exists())\n",
    "    print(\"Found directory {} containing class {}\".format(cdir,names[cdir_class]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to read the first image from the first directory, and visualize it.  Note that the tool allows you to zoom in order to see the individual pixels.\n",
    "\n",
    "This cell can be safely skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = skimage.io.imread(list(list(cdirs.keys())[0].glob(\"*\"))[0])\n",
    "viewer=skimage.viewer.ImageViewer(im)\n",
    "viewer.show()\n",
    "# Note: you have to close the window to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now read in all images, cut the central square (with an edge as long as the shorter dimension), and resize it to 500x500 pixels.  Whatever the initial size and orientation of the images, we will end up with a bunch of 500x500 RGB squares in uint8.  These should be small enough that unless the dataset is huge, all should fit in memory.\n",
    "\n",
    "We make a pandas dataframe with the data, with two columns:\n",
    "* image: a $500 \\times 500 \\times 3$ uint8 numpy array\n",
    "* label: on of 0, 1 or 2\n",
    "* file: the full path of the image\n",
    "\n",
    "Note: this step may take a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e8f20f89b41a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcdir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcdirs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcdir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "imagesize = 500\n",
    "\n",
    "dataset=[]\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "for cdir,cn in cdirs.items():\n",
    "    for f in tqdm(list(cdir.glob(\"*\"))):\n",
    "        \n",
    "        im=skimage.io.imread(f)\n",
    "        h,w=im.shape[0:2] # height, width\n",
    "        sz=min(h,w)\n",
    "        im=im[(h//2-sz//2):(h//2+sz//2),(w//2-sz//2):(w//2+sz//2),:] # defines the central square\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            im=skimage.img_as_ubyte(skimage.transform.resize(im,(imagesize,imagesize))) # resize it to 500x500, whatever the original resolution\n",
    "        \n",
    "        dataset.append({\n",
    "            \"file\":f,\n",
    "            \"label\":cn,\n",
    "            \"image\":im})\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a pandas dataframe for the dataset, and create a \"dn\" field containing the name of the dataset from which each image comes (as the name of the directory it was read from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame(dataset)\n",
    "dataset[\"dn\"]=dataset[\"file\"].apply(lambda x: x.parent.parts[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are 10 random rows from that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.sample(n=10)[[\"image\",\"label\",\"file\",\"dn\"]].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can quickly scroll through the images in our dataset.\n",
    "\n",
    "This cell can be safely skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer=skimage.viewer.CollectionViewer([r[1][\"image\"] for r in dataset.iterrows()])\n",
    "viewer.show()\n",
    "\n",
    "# Note: you have to close the window to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare training and testing sets\n",
    "\n",
    "How should we split training and testing data?  The code below implements a few options (run only one of the cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option one (easy): use all samples from dataset named \"testing\"\n",
    "# which contains some images taken in the same days as D1--D4, but not contained in these dirs.\n",
    "te_mask = dataset[\"dn\"]==\"testing\"\n",
    "dataset_te=dataset[te_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Option two (hard): use all samples from dataset named \"D5\"\n",
    "te_mask = dataset[\"dn\"]==\"D5\"\n",
    "dataset_te=dataset[te_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In any case, training samples are all other samples\n",
    "dataset_tr=dataset.loc[dataset.index.difference(dataset_te.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of how many training and testing images we have sampled\n",
    "import collections\n",
    "pd.DataFrame(index=[0,1,2],data=collections.OrderedDict((\n",
    "    (\"Class name\",           names),\n",
    "    (\"# Training images\", dataset_tr[\"label\"].value_counts()),\n",
    "    (\"# Testing images\",  dataset_te[\"label\"].value_counts())))).set_index(\"Class name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "examples=list(dataset_tr[\"image\"])\n",
    "interact(\n",
    "    imgplotList, \n",
    "    i=widgets.IntSlider(min=0,max=len(examples)-1, step=1, value=0,continuous_update=False), \n",
    "    data=fixed(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: define what we feed to the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take image and resize to a specified size\n",
    "def transform_simple(im,sz):\n",
    "    imr = skimage.transform.resize(im, (sz,sz))\n",
    "    return imr\n",
    "\n",
    "# Take image and resize to a specified size, after applying data augmentation\n",
    "def transform_complex(im,sz):\n",
    "    if(np.random.rand()<0.5):\n",
    "        im=np.fliplr(im)\n",
    "    tf1 = skimage.transform.SimilarityTransform(scale = 1 / im.shape[0])\n",
    "    tf2 = skimage.transform.SimilarityTransform(translation=[-0.5, -0.5])\n",
    "    tf3 = skimage.transform.SimilarityTransform(rotation=np.deg2rad(np.random.uniform(0,360)))\n",
    "    tf4 = skimage.transform.SimilarityTransform(scale=np.random.uniform(1,1.6))\n",
    "    tf5 = skimage.transform.SimilarityTransform(translation=np.array([0.5, 0.5])+np.random.uniform(-0.1,0.1,size=2))\n",
    "    tf6 = skimage.transform.SimilarityTransform(scale=sz)\n",
    "    imr = skimage.transform.warp(im, (tf1+(tf2+(tf3+(tf4+(tf5+tf6))))).inverse, output_shape=(sz,sz),mode=\"edge\")\n",
    "    imr = imr*np.random.uniform(0.9,1.1,size=(1,1,3))\n",
    "    imr = np.clip(imr,0,1)\n",
    "    return imr\n",
    "\n",
    "transform=transform_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(df,sz):\n",
    "    r=df.sample(n=1)\n",
    "    l=r[\"label\"].iloc[0]\n",
    "    im=r[\"image\"].iloc[0]\n",
    "    im=transform(im,sz)\n",
    "    return im,l\n",
    "\n",
    "def mktrte(df,N,sz):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        im,l=sample(df,sz)\n",
    "        X.append(im)\n",
    "        y.append(l)\n",
    "    X=np.array(X).astype('float32')\n",
    "    y=np.array(y)\n",
    "    y=keras.utils.np_utils.to_categorical(y,3)\n",
    "    return X,y\n",
    "\n",
    "def generator(df,batch_size,sz):\n",
    "    while True:\n",
    "        X,y = mktrte(df,batch_size,sz)\n",
    "        yield (X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 100 images from our input dataset\n",
    "examples = list(mktrte(dataset_tr,100,patchsize)[0])\n",
    "interact(imgplotList, i=widgets.IntSlider(min=0, max=len(examples)-1, step=1, value=0,continuous_update=False), data=fixed(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize 100 variations from our first input image (makes sense only if transform==transform_complex, i.e. if we are using data augmentation)\n",
    "examples = list(mktrte(dataset_tr.iloc[[0]],100,patchsize)[0])\n",
    "interact(imgplotList, i=widgets.IntSlider(min=0, max=len(examples)-1, step=1, value=0,continuous_update=False), data=fixed(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: build and train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras: Deep Learning library for Theano and TensorFlow\n",
    "import keras\n",
    "from keras.utils  import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "# NN settings\n",
    "patchsize          = 64\n",
    "batch_size         = 20\n",
    "nb_filters         = 5     # number of convolutional filters to use\n",
    "pool_size          = (2,2) # size of pooling area for max pooling\n",
    "kernel_size        = (3,3) # convolution kernel size\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(nb_filters, kernel_size, padding='valid', input_shape=(patchsize,patchsize,3))) # 3 perchè rgb\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(nb_filters, kernel_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Conv2D(nb_filters, kernel_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Conv2D(nb_filters, kernel_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128)) # generate a fully connected layer wiht 128 outputs (arbitrary value)\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3)) # output layer\n",
    "model.add(Activation('softmax')) # output activation per renderle probabilità\n",
    "\n",
    "## compile! network\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"adadelta\",\n",
    "              metrics=['accuracy'])\n",
    "print(model.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a set of 1000 testing instances taken from the testing dataset.\n",
    "\n",
    "Note: \"testing\" is synonym with \"validation\" and \"evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_test,y_test) = mktrte(dataset_te, 1000, patchsize)\n",
    "\n",
    "# Prepare the logs directory, if it does not exist\n",
    "(pathlib.Path(\".\")/\"logs\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the network trains, we can monitor training loss/accuracy and testing loss/accuracy using tensorboard at http://0.0.0.0:6006\n",
    "\n",
    "You may need to launch tensorboard first if it's not already running, by executing\n",
    "\n",
    "`tensorboard --logdir=logs`\n",
    "\n",
    "in a shell with the current working directory.  Check that you are within the proper conda environment, if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit_generator(\n",
    "                    generator(dataset_tr, batch_size, patchsize),\n",
    "                    steps_per_epoch=50, \n",
    "                    epochs=5000, \n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test,y_test),\n",
    "                    callbacks=[keras.callbacks.TensorBoard(log_dir='./logs/'+time.strftime(\"%Y%m%d%H%M%S\"), histogram_freq=0, write_graph=False, write_images=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we can save it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelname = \"myModel\"\n",
    "\n",
    "# Save model to a file\n",
    "keras.models.save_model(model,\"{}.model\".format(modelname))\n",
    "\n",
    "# Also save the testing dataset (may be large) so we can pick up from here later \n",
    "dataset_te.to_pickle(\"{}.testingdata.pickle\".format(modelname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Process the images of the testing set one by one\n",
    "And visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optionally, load a saved model and testing dataset\n",
    "modelname = \"models/model_venus\"\n",
    "\n",
    "model = keras.models.load_model(\"{}.model\".format(modelname))\n",
    "dataset_te = pd.read_pickle(\"{}.testingdata.pickle\".format(modelname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results by processing a single variation of the testing image\n",
    "\n",
    "%matplotlib inline\n",
    "def resultsShow(i, data, model):\n",
    "    d = data.iloc[i]\n",
    "    im = d[\"image\"]\n",
    "    l = d[\"label\"]\n",
    "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(15,5),gridspec_kw={'width_ratios':[1,1,0.5]})\n",
    "    \n",
    "    imt = transform_simple(im, patchsize)\n",
    "    axs[0].imshow(im)\n",
    "    axs[0].set_title(\"Image (true class: {})\".format(names[l]))\n",
    "    \n",
    "    axs[1].imshow(imt,interpolation=\"nearest\")\n",
    "    axs[1].set_title(\"Network input\")\n",
    "    \n",
    "    outs = model.predict(np.array([imt]))\n",
    "    print(outs)\n",
    "    predicted = np.argmax(outs)\n",
    "    axs[2].bar(np.array(range(len(names)))-0.5, outs[0,:], 1, color=\"gray\")\n",
    "    axs[2].set_ylim([0,1])\n",
    "    axs[2].set_xticks(range(len(names)))\n",
    "    axs[2].set_xticklabels(names)\n",
    "    axs[2].set_ylabel(\"probability\")\n",
    "    axs[2].set_xlabel(\"class\")\n",
    "    axs[2].set_title(\"Network output\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(\"out_{:05d}_{}.png\".format(i,(\"ok\" if predicted==l else \"ko\")))\n",
    "    print(outs)\n",
    "\n",
    "interact(resultsShow, i=widgets.IntSlider(min=0,max=len(dataset_te)-1, step=1, value=0, continuous_update=False), data=fixed(dataset_te.sample(len(dataset_te))), model=fixed(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can also visualize the contents of the hidden layers of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def plot_hidden_layers(imt,nmaps=4):\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    plt.title('Hidden layers', loc='center')\n",
    "    plt.axis('off')\n",
    "    layers = [model.layers[0].input] + [model.layers[i].output for i in [0,2,5,7,9]]\n",
    "    for layeri,layer in enumerate(layers):\n",
    "        get = K.function([model.layers[0].input], [layer])\n",
    "        layeroutputs = get([imt[np.newaxis,:,:,:]])[0][0]\n",
    "        for j in range(nmaps): # for each map\n",
    "            sp2=fig.add_subplot(nmaps,len(layers),layeri+1+j*len(layers))\n",
    "            sp2.axis('off')\n",
    "            if (layeroutputs.shape[2]>j):\n",
    "                vmin,vmax = ((0,1) if layeri == 0 else (-0.3,+0.3))\n",
    "                sp2.imshow(layeroutputs[:,:,j],\n",
    "                           cmap=\"gray\",\n",
    "                           interpolation=\"nearest\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "imt=transform(dataset_te[\"image\"].iloc[1],patchsize)\n",
    "plot_hidden_layers(imt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
